# Human Pose Estimation with Ultralytics YOLO (COMPE 510 Term Project)

This project fine-tunes an Ultralytics YOLO pose model on the **TrainingDataPro Human Pose Estimation** dataset and adds:

- A **Stage 1** model training pipeline + simple inference script  
- A **GUI** for visualizing predicted human poses  
- A **Stage 2** model compression pipeline (pruning + FP16 inference)  
- A **comparison GUI** to visualize pruned FP32 vs pruned FP16 in real time

---

## 1. Folder Structure

Recommended project layout:

```text
pose-estimation-yolo/
├── configs/
│   ├── pose_estimation.yaml         # YOLO dataset config
│   └── optimize_pose.yaml           # Stage 2 pruning + FP16 config
├── data/
│   ├── kaggle_raw/
│   │   ├── annotations.xml          # From Kaggle dataset
│   │   └── images/                  # Renamed from original 'PE' folder
│   │       ├── 0.jpg
│   │       ├── 1.jpg
│   │       └── ...
│   └── yolo_format/                 # Auto-generated by convert_kaggle_to_yolo.py
│       ├── images/
│       │   ├── train/
│       │   └── val/
│       └── labels/
│           ├── train/
│           └── val/
├── runs/
│   └── pose/
│       └── pose-estimation-yolo/
│           └── weights/
│               ├── best.pt          # Trained baseline model (Stage 1)
│               ├── best_pruned.pt   # Pruned model (Stage 2)
│               └── best_pruned_fp16.pt # FP16 checkpoint if you export it
├── src/
│   ├── convert_kaggle_to_yolo.py    # Convert annotations.xml + images -> YOLO pose format
│   ├── train_yolo_pose.py           # Fine-tune YOLO pose model (Stage 1)
│   ├── infer_single_image.py        # CLI inference on a single image
│   ├── optimize_pose_model.py       # Pruning + FP16 benchmarking (Stage 2)
│   └── gui_app.py                   # GUI to compare pruned FP32 vs FP16
├── requirements.txt
└── README.md                        # This file
```

> **Note:**  
> `data/yolo_format/` is **generated automatically** by `convert_kaggle_to_yolo.py`.  
> You do **not** manually put images or labels there.

---

## 2. Setup Instructions

### 2.1. Create and activate a virtual environment (optional but recommended)

```bash
cd pose-estimation-yolo

python -m venv venv
# macOS / Linux:
source .venv/bin/activate
# Windows (PowerShell):
# venv\Scripts\Activate.ps1
```

### 2.2. Install dependencies

Make sure `requirements.txt` contains at least:

```txt
ultralytics>=8.2.0
opencv-python
numpy
pandas
Pillow
tqdm
pyyaml
```

Then run:

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

---

## 3. Prepare the Dataset

### 3.1. Download Kaggle dataset

Download the **TrainingDataPro Human Pose Estimation** dataset from Kaggle. After unzipping it, you should have:

- A folder named `PE` with all images.
- A file `annotations.xml` with pose annotations.

Place them into:

```text
pose-estimation-yolo/
└── data/
    └── kaggle_raw/
        ├── annotations.xml
        └── PE/
            ├── 0.jpg
            ├── 1.jpg
            └── ...
```

Rename `PE` to `images` (or update the script if you choose a different name):

```text
pose-estimation-yolo/
└── data/
    └── kaggle_raw/
        ├── annotations.xml
        └── images/
            ├── 0.jpg
            ├── 1.jpg
            └── ...
```

The converter script assumes `data/kaggle_raw/images/` by default.

---

## 4. Configure the Dataset for YOLO

### 4.1. Create `configs/pose_estimation.yaml`

Create the folder and file:

```bash
mkdir -p configs
```

`configs/pose_estimation.yaml`:

```yaml
# Ultralytics YOLO pose dataset config for TrainingDataPro Human Pose Estimation

path: data/yolo_format   # root relative to project root
train: images/train
val: images/val
test:

# Keypoints
kpt_shape: [17, 3]  # 17 keypoints, (x, y, visibility)
flip_idx: [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]

# Classes
names:
  0: person

# Keypoint names (COCO order)
kpt_names:
  0:
    - nose
    - left_eye
    - right_eye
    - left_ear
    - right_ear
    - left_shoulder
    - right_shoulder
    - left_elbow
    - right_elbow
    - left_wrist
    - right_wrist
    - left_hip
    - right_hip
    - left_knee
    - right_knee
    - left_ankle
    - right_ankle
```

This tells YOLO where your images and labels live and defines the pose keypoint layout.

---

## 5. Stage 1 – Data Conversion & Training

### 5.1. Convert `annotations.xml` to YOLO pose format

`src/convert_kaggle_to_yolo.py`:

- Reads `data/kaggle_raw/annotations.xml` and `data/kaggle_raw/images/`.
- Maps the dataset’s 18 keypoints to COCO 17 keypoints.
- Splits data into **train (80%)** and **val (20%)**.
- Writes YOLO pose labels and copies images into `data/yolo_format`.

Run:

```bash
python src/convert_kaggle_to_yolo.py
```

After it finishes, you should see:

```text
data/yolo_format/
  images/train/*.jpg
  images/val/*.jpg
  labels/train/*.txt
  labels/val/*.txt
```

Each `.txt` file corresponds to one image (YOLO pose format).

---

### 5.2. Train the YOLO pose model

`src/train_yolo_pose.py`:

- Loads a pretrained YOLO pose model (e.g. `yolov8n-pose.pt`).
- Fine-tunes it on your converted dataset.
- Saves checkpoints under `runs/pose/pose-estimation-yolo/weights/`.

Run:

```bash
python src/train_yolo_pose.py
```

When training finishes, look for:

```text
runs/pose/pose-estimation-yolo/weights/best.pt
```

This is your **trained Stage 1 model**.

---

### 5.3. Test inference on a single image (optional sanity check)

`src/infer_single_image.py`:

```bash
python src/infer_single_image.py   --model runs/pose/pose-estimation-yolo/weights/best.pt   --image data/yolo_format/images/val/<some_image>.jpg   --out outputs/test_annotated.png
```

Check `outputs/test_annotated.png` to confirm the pose drawing looks correct.

---

## 6. Stage 2 – Pruning + FP16 Inference (Optional, for Compression Study)

Stage 2 is **post-training compression**:

- Apply **unstructured pruning** to the trained model.
- Run inference in **FP16** to reduce compute and measure speed/accuracy trade-offs.
- Log metrics (sparsity, file size, latency, mAP) to a CSV.

### 6.1. Configure Stage 2

Create `configs/optimize_pose.yaml` (if not already present), with content similar to:

```yaml
# configs/optimize_pose.yaml

# ---------- Paths ----------
base_model: runs/pose/pose-estimation-yolo/weights/best.pt
data_yaml: configs/pose_estimation.yaml
device: "cpu"           # or "0" / "cuda" if you have GPU

imgsz: 640
batch: 8

# Directory with validation images (from the converter)
val_images_dir: data/yolo_format/images/val

# ---------- Which experiments to run ----------
experiments:
  baseline_fp32: true
  pruned_fp32: true
  pruned_fp16: true

# ---------- Pruning config ----------
pruning:
  enabled: true
  global_amount: 0.3              # 30% global unstructured pruning
  module_types: ["Conv2d", "Linear"]

# ---------- Quantization config ----------
quantization:
  enabled: true
  type: "fp16"

# ---------- Benchmark config ----------
benchmark:
  num_warmup: 5
  num_iters: 30
  num_images: 50
  results_csv: runs/pose/optimization_results.csv
```

Adjust paths and pruning amount as needed.

### 6.2. Run optimization experiments

```bash
python src/optimize_pose_model.py --config configs/optimize_pose.yaml
```

This will:

- Evaluate **baseline_fp32** (`best.pt`)
- Create and evaluate **pruned_fp32** (`best_pruned.pt`)
- Evaluate **pruned_fp16** (FP16 inference based on pruned weights)

Results are written to:

```text
runs/pose/optimization_results.csv
```

You can use this CSV in your report to discuss:

- Sparsity vs accuracy.
- Latency (ms/image) vs accuracy.
- Size on disk (MB) for each variant.

---

## 7. GUI – Visual Demo

There are two possible GUI modes in this project:

1. **Single-model GUI (simple demo)** – load any `.pt` and run pose on images.  
2. **Comparison GUI** – compare `best_pruned.pt` vs `best_pruned_fp16.pt` side-by-side.

The final version (`src/gui_app.py`) is the **comparison GUI**.

### 7.1. Prerequisites

Make sure you have at least:

```text
runs/pose/pose-estimation-yolo/weights/best_pruned.pt
```

Optionally (if you exported a separate one):

```text
runs/pose/pose-estimation-yolo/weights/best_pruned_fp16.pt
```

If `best_pruned_fp16.pt` doesn’t exist, the GUI will still work; it will run the pruned model with `half=True` to simulate FP16 inference.

### 7.2. Run the comparison GUI

```bash
python src/gui_app.py
```

Or explicitly:

```bash
python src/gui_app.py   --model_pruned runs/pose/pose-estimation-yolo/weights/best_pruned.pt   --model_pruned_fp16 runs/pose/pose-estimation-yolo/weights/best_pruned_fp16.pt
```

In the GUI:

1. Click **“Open Image”** and select an image containing a person.
2. Click **“Run Pruned FP32”**  
   - Left panel shows the output of `best_pruned.pt` (FP32).
3. Click **“Run Pruned FP16”**  
   - Right panel shows the output of the FP16 variant (or pruned model run with `half=True`).

You can now **visually compare** pose predictions between pruned FP32 and pruned FP16 in real time for your term project demo.

---

## 8. Typical Workflow Summary

1. **Set up environment**  
   - Create venv (optional)  
   - `pip install -r requirements.txt`

2. **Prepare data**  
   - Download Kaggle Human Pose Estimation dataset  
   - Place `annotations.xml` and images under `data/kaggle_raw/`  
   - Ensure images folder is called `images/`

3. **Convert dataset**  
   - `python src/convert_kaggle_to_yolo.py`

4. **Train model (Stage 1)**  
   - `python src/train_yolo_pose.py`

5. **(Optional) Quick inference check**  
   - `python src/infer_single_image.py --image <path> --out outputs/test.png`

6. **Run compression experiments (Stage 2)**  
   - Configure `configs/optimize_pose.yaml`  
   - `python src/optimize_pose_model.py --config configs/optimize_pose.yaml`

7. **Run GUI to demo results**  
   - `python src/gui_app.py`  
   - Compare pruned FP32 vs pruned FP16 visually.

That’s it! This README should be enough for someone else to clone your repo, follow the steps, and reproduce both training and the GUI demo for your COMPE 510 term project.
